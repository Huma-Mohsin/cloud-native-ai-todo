# Test Suite Documentation - Phase III AI-Powered Chatbot

**Generated by**: `/sp.test-suite`
**Feature**: 003-phase-iii-chatbot
**Date**: 2026-01-18

---

## Overview

Comprehensive test suite for the AI-Powered Chatbot feature, covering:
- ✅ **Unit Tests**: Models, services, MCP tools (80%+ coverage target)
- ✅ **Integration Tests**: API endpoints, database operations (100% endpoint coverage)
- ✅ **E2E Tests**: Complete user workflows (critical paths)
- ✅ **Load Tests**: Performance benchmarks (Locust & K6)
- ✅ **CI/CD Integration**: Automated testing pipeline

---

## Test Structure

```
tests/
├── unit/                    # Unit tests (fast, isolated)
│   ├── test_conversation.py    # Conversation model tests
│   ├── test_message.py          # Message model tests
│   └── test_mcp_tools.py        # MCP tool unit tests
│
├── integration/             # Integration tests (database + API)
│   ├── test_conversation_crud.py  # Conversation CRUD tests
│   ├── test_mcp_add_task.py       # Add task tool integration
│   ├── test_mcp_list_tasks.py     # List tasks tool integration
│   ├── test_mcp_complete_task.py  # Complete task tool integration
│   ├── test_mcp_delete_task.py    # Delete task tool integration
│   ├── test_mcp_update_task.py    # Update task tool integration
│   └── test_chat_api.py           # Chat endpoint integration
│
├── e2e/                     # End-to-end tests (full workflows)
│   └── test_chat_flows.py         # Complete conversation flows
│
├── load/                    # Load testing
│   ├── locustfile.py              # Locust load tests
│   └── k6_script.js               # K6 load tests
│
└── fixtures/                # Test data and utilities
    └── (shared test data)
```

---

## Running Tests

### Quick Start

```bash
# Backend: All tests
cd phase-3-ai-chatbot/backend
pytest

# Backend: Specific test type
pytest tests/unit -v           # Unit tests only
pytest tests/integration -v    # Integration tests only
pytest tests/e2e -v            # E2E tests only

# Backend: With coverage
pytest --cov=src --cov-report=html
open htmlcov/index.html

# Frontend: All tests
cd phase-3-ai-chatbot/frontend
npm test

# Frontend: Coverage
npm test -- --coverage
```

### Detailed Commands

#### Backend Unit Tests

```bash
# All unit tests
pytest tests/unit -v

# Specific test file
pytest tests/unit/test_conversation.py -v

# Specific test function
pytest tests/unit/test_conversation.py::test_create_conversation -v

# With markers
pytest -m "unit and chat" -v

# Fast parallel execution
pytest tests/unit -n auto
```

#### Backend Integration Tests

```bash
# All integration tests
pytest tests/integration -v

# MCP tool tests only
pytest tests/integration -k "mcp" -v

# Chat API tests
pytest tests/integration/test_chat_api.py -v

# With timeout
pytest tests/integration --timeout=60
```

#### Backend E2E Tests

```bash
# All E2E tests
pytest tests/e2e -v

# Specific workflow
pytest tests/e2e/test_chat_flows.py::test_e2e_create_task_flow -v

# With detailed output
pytest tests/e2e -v -s
```

#### Load Tests

**Locust (Python)**:
```bash
cd tests/load

# Interactive mode (web UI)
locust -f locustfile.py --host=http://localhost:8000
# Open browser to http://localhost:8089

# Headless mode
locust -f locustfile.py --headless \
  -u 100 \                    # 100 users
  -r 10 \                     # 10 users/second spawn rate
  -t 60s \                    # Run for 60 seconds
  --host=http://localhost:8000

# Specific user class
locust -f locustfile.py --headless \
  -u 50 -r 5 -t 30s \
  ChatbotUser
```

**K6 (JavaScript)**:
```bash
cd tests/load

# Run with default config
k6 run k6_script.js

# Custom configuration
k6 run --vus 50 --duration 60s k6_script.js

# With environment variable
K6_BASE_URL=http://localhost:8000 k6 run k6_script.js

# Output to file
k6 run --out json=results.json k6_script.js
```

---

## Test Coverage

### Current Coverage

Run to check current coverage:
```bash
pytest --cov=src --cov-report=term-missing
```

### Coverage Targets

| Component | Target | Status |
|-----------|--------|--------|
| **Backend Overall** | 80% | ⏳ In Progress |
| MCP Tools | 100% | ✅ Complete |
| Models (Conversation, Message) | 100% | ✅ Complete |
| Services | 90% | ⏳ In Progress |
| API Endpoints | 100% | ⏳ In Progress |
| **Frontend Overall** | 70% | ⏳ Pending |
| Components | 80% | ⏳ Pending |
| Services | 90% | ⏳ Pending |

---

## Writing New Tests

### Backend Unit Test Template

```python
"""Unit tests for [component name]."""

import pytest
from src.models.example import Example

@pytest.mark.asyncio
async def test_example_feature(async_session, test_user_id):
    """Test [specific feature].

    Acceptance Criteria:
    - Feature works as expected
    - Edge cases are handled
    - Errors are caught
    """
    # Arrange
    example = Example(user_id=test_user_id, title="Test")

    # Act
    async_session.add(example)
    await async_session.commit()
    await async_session.refresh(example)

    # Assert
    assert example.id is not None
    assert example.title == "Test"
```

### Backend Integration Test Template

```python
"""Integration tests for [API endpoint]."""

import pytest
from src.mcp.tools.example import example_handler
from src.mcp.schemas import ExampleInput

@pytest.mark.asyncio
async def test_example_integration(async_session):
    """Test [endpoint] integration.

    Acceptance Criteria:
    - Request is processed correctly
    - Database is updated
    - Response is formatted correctly
    """
    # Arrange
    input_data = ExampleInput(user_id=1, title="Test")

    # Act
    result = await example_handler(input_data, async_session)

    # Assert
    assert result.success is True
    assert result.title == "Test"
```

### Frontend Unit Test Template

```typescript
import { render, screen, waitFor } from '@testing-library/react';
import userEvent from '@testing-library/user-event';
import ExampleComponent from '@/components/ExampleComponent';

describe('ExampleComponent', () => {
  it('should render correctly', () => {
    render(<ExampleComponent />);
    expect(screen.getByText('Expected Text')).toBeInTheDocument();
  });

  it('should handle user interaction', async () => {
    const user = userEvent.setup();
    render(<ExampleComponent />);

    await user.click(screen.getByRole('button', { name: 'Click Me' }));

    await waitFor(() => {
      expect(screen.getByText('Clicked!')).toBeInTheDocument();
    });
  });
});
```

---

## Test Markers

Use pytest markers to categorize tests:

```python
@pytest.mark.unit         # Unit test (fast)
@pytest.mark.integration  # Integration test
@pytest.mark.e2e          # End-to-end test
@pytest.mark.slow         # Slow test (>1s)
@pytest.mark.chat         # Chat-related test
@pytest.mark.mcp          # MCP tool test
@pytest.mark.auth         # Authentication test
```

Run tests by marker:
```bash
pytest -m unit          # Only unit tests
pytest -m "not slow"    # Skip slow tests
pytest -m "chat and mcp"  # Chat + MCP tests
```

---

## CI/CD Integration

Tests run automatically on:
- ✅ Every push to `main`, `develop`, `003-phase-iii-chatbot`
- ✅ Every pull request to `main`, `develop`
- ✅ Changes to `phase-3-ai-chatbot/**`

### GitHub Actions Workflow

See `.github/workflows/test-phase3-chatbot.yml` for CI/CD configuration.

**Jobs**:
1. `backend-unit-tests` - Unit tests with coverage
2. `backend-integration-tests` - Integration tests
3. `backend-e2e-tests` - E2E workflow tests
4. `frontend-tests` - Frontend unit tests
5. `load-tests` - Performance benchmarks (main branch only)
6. `all-tests-passed` - Final check

**Failed tests block deployment** to ensure quality.

---

## Test Data & Fixtures

### Backend Fixtures (conftest.py)

Available fixtures:
- `async_session` - Database session for tests
- `test_user_id` - Test user ID (`auth0|test_user_123`)
- `test_conversation` - Pre-created conversation
- `test_task` - Pre-created task

Example usage:
```python
@pytest.mark.asyncio
async def test_with_fixtures(async_session, test_user_id, test_conversation):
    # Use fixtures directly
    message = Message(
        conversation_id=test_conversation.id,
        user_id=test_user_id,
        role="user",
        content="Test message"
    )
    async_session.add(message)
    await async_session.commit()
```

---

## Performance Benchmarks

### Target Response Times

| Operation | Target (p95) | Current |
|-----------|--------------|---------|
| Chat endpoint | < 2000ms | ⏳ TBD |
| MCP tool execution | < 200ms | ⏳ TBD |
| Database queries | < 100ms | ⏳ TBD |

### Load Test Scenarios

**Locust Scenarios**:
- `ChatbotUser`: Realistic user behavior (5 tasks, 2-5s wait)
- `StressTestUser`: Rapid requests (0.1-0.5s wait)

**K6 Scenarios**:
- Ramp-up: 0 → 10 → 50 → 100 users
- Spike test: Sudden 100 users
- Soak test: 50 users for 2 minutes

---

## Troubleshooting

### Common Issues

**Issue**: Tests fail with `pydantic validation error: user_id should be string`
```bash
# Solution: Ensure user_id is string in MCP tests
input_data = AddTaskInput(user_id="1", title="Test")  # ✅ String
input_data = AddTaskInput(user_id=1, title="Test")    # ❌ Integer
```

**Issue**: Async tests hang
```bash
# Solution: Use pytest-asyncio markers
@pytest.mark.asyncio
async def test_example():
    ...
```

**Issue**: Database connection errors
```bash
# Solution: Check async_session fixture is used
async def test_example(async_session):  # ✅ Use fixture
    ...
```

### Debug Mode

Run tests with verbose output:
```bash
pytest -v -s --log-cli-level=DEBUG
```

---

## Best Practices

✅ **DO**:
- Write descriptive test names (`test_create_task_success` not `test1`)
- Use AAA pattern (Arrange, Act, Assert)
- Test one thing per test
- Use fixtures for setup/teardown
- Add docstrings with acceptance criteria
- Mock external dependencies
- Test edge cases and error paths

❌ **DON'T**:
- Write tests that depend on each other
- Use real external APIs in tests
- Hardcode test data
- Skip cleanup (use fixtures)
- Test implementation details
- Write flaky tests

---

## Resources

- [Pytest Documentation](https://docs.pytest.org/)
- [Testing Library](https://testing-library.com/)
- [Locust Documentation](https://docs.locust.io/)
- [K6 Documentation](https://k6.io/docs/)
- [GitHub Actions](https://docs.github.com/en/actions)

---

## Next Steps

1. **Install Dependencies**:
   ```bash
   # Backend
   pip install pytest pytest-cov pytest-asyncio pytest-timeout locust

   # Frontend
   npm install --save-dev @testing-library/react @testing-library/jest-dom
   ```

2. **Run Tests**:
   ```bash
   # Verify all tests pass
   pytest
   npm test
   ```

3. **Check Coverage**:
   ```bash
   pytest --cov=src --cov-report=html
   open htmlcov/index.html
   ```

4. **Run Load Tests** (after backend is running):
   ```bash
   locust -f tests/load/locustfile.py --host=http://localhost:8000
   ```

---

**Questions?** See the main project documentation or contact the team.

**Generated with** ❤️ **by `/sp.test-suite` custom skill**
